# -*- coding: utf-8 -*-
"""
ì¤‘ê³ ë‚˜ë¼ ê²€ìƒ‰ ê¸°ë°˜ í¬ë¡¤ë§ (Selenium ë²„ì „)
- Seleniumì„ ì‚¬ìš©í•˜ì—¬ JavaScript ë Œë”ë§ í›„ HTML íŒŒì‹±
- ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì—ì„œ ìƒí’ˆ ì¹´ë“œ(li)ë³„ë¡œ
  ë§í¬ / ìœ„ì¹˜ / ì‹œê°„ ìˆ˜ì§‘
- ê° ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ë¥¼ íŒŒì‹±í•´ì„œ name / price ì¶”ì¶œ
- ìµœì¢…ì ìœ¼ë¡œ name / price / location / time / link ë¥¼ CSVë¡œ ì €ì¥
"""

import re
import os
import json
import time
import random
import argparse
from typing import Optional, List, Dict, Tuple
from urllib.parse import quote, urljoin

import requests
import pandas as pd
from bs4 import BeautifulSoup

# Selenium imports
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

# ===== ì„¤ì • =====

BASE_SEARCH_URL = "https://web.joongna.com/search/{keyword}?keywordSource=INPUT_KEYWORD"
# ì¤‘ê³ ë‚˜ë¼ API ì—”ë“œí¬ì¸íŠ¸
API_SEARCH_URL = "https://api.joongna.com/v3/search/products"

USER_AGENT = (
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
)
HEADERS = {
    "User-Agent": USER_AGENT,
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    "Referer": "https://web.joongna.com/",
}

API_HEADERS = {
    "User-Agent": USER_AGENT,
    "Accept": "application/json",
    "Accept-Language": "ko-KR,ko;q=0.9",
    "Origin": "https://web.joongna.com",
    "Referer": "https://web.joongna.com/",
}

PRICE_PAT = re.compile(r"(\d{1,3}(?:,\d{3})*|\d+)\s*ì›")
# ìˆ«ì(ì˜ë¬¸/ì „ê° ëª¨ë‘) + ë‹¨ìœ„ + "ì „"
TIME_PAT = re.compile(r"[0-9ï¼-ï¼™]+\s*(ì´ˆ|ë¶„|ì‹œê°„|ì¼|ì£¼|ê°œì›”|ë‹¬)\s*ì „")


# ================== Selenium WebDriver ì„¤ì • ==================


def create_driver(headless: bool = True) -> webdriver.Chrome:
    """
    Chrome WebDriver ìƒì„±
    - headless: Trueë©´ ë¸Œë¼ìš°ì € ì°½ì„ ë„ìš°ì§€ ì•ŠìŒ
    """
    chrome_options = Options()

    if headless:
        chrome_options.add_argument("--headless")

    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument(f"user-agent={USER_AGENT}")
    chrome_options.add_argument("--window-size=1920,1080")

    # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ
    prefs = {"profile.managed_default_content_settings.images": 2}
    chrome_options.add_experimental_option("prefs", prefs)

    driver = webdriver.Chrome(options=chrome_options)
    driver.implicitly_wait(10)

    return driver


# ================== JSON-LD / ê°€ê²© íŒŒì„œ ==================


def _parse_jsonld_product(soup: BeautifulSoup) -> dict:
    """JSON-LD(Product)ì˜ name/price/seller/date* ë“±ì„ ì¶”ì¶œ."""
    out = {"name": None, "price": None, "seller": None, "date": None}
    for tag in soup.find_all("script", type="application/ld+json"):
        raw = tag.string or ""
        try:
            data = json.loads(raw)
        except Exception:
            continue

        items = data if isinstance(data, list) else [data]
        for obj in items:
            if not isinstance(obj, dict):
                continue

            # datePublished/Modified/uploadDate/releaseDate ì¤‘ íƒ1
            for date_key in ("datePublished", "dateModified", "uploadDate", "releaseDate"):
                if obj.get(date_key):
                    out["date"] = str(obj[date_key])
                    break

            if obj.get("@type") == "Product":
                out["name"] = out["name"] or obj.get("name")
                offers = obj.get("offers")
                price = None
                seller = None
                if isinstance(offers, dict):
                    price = offers.get("price")
                    seller = (offers.get("seller") or {}).get("name")
                elif isinstance(offers, list) and offers:
                    price = offers[0].get("price")
                    seller = (offers[0].get("seller") or {}).get("name")

                try:
                    price = int(str(price).replace(",", "")) if price is not None else None
                except Exception:
                    price = None

                out["price"] = price if out["price"] is None else out["price"]
                out["seller"] = seller if out["seller"] is None else out["seller"]

    return out


def _extract_price_from_text(soup: BeautifulSoup) -> Optional[int]:
    """ì—¬ëŸ¬ í›„ë³´ ë…¸ë“œì™€ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ê°€ê²© ì •ê·œì‹ìœ¼ë¡œ ë°±ì—… ì¶”ì¶œ."""
    candidates: List[str] = []
    candidates += [
        el.get_text(" ", strip=True)
        for el in soup.select(
            "div[class*='price'], span[class*='price'], div.font-semibold"
        )
    ]
    candidates.append(soup.get_text(" ", strip=True)[:8000])

    for txt in candidates:
        txt = (txt or "").replace("\u00a0", " ")
        m = PRICE_PAT.search(txt)
        if m:
            try:
                return int(m.group(1).replace(",", ""))
            except Exception:
                pass
    return None


# ================== ì‹œê°„/ìœ„ì¹˜ íŒë³„ ìœ í‹¸ ==================


def looks_like_time(text: str) -> bool:
    """'32ë¶„ ì „' ê°™ì€ ì‹œê°„ ë¬¸ìì—´ì¸ì§€ ëŒ€ì¶© íŒë‹¨."""
    text = text.replace("\u00a0", " ").strip()
    if "ì „" not in text:
        return False
    if not TIME_PAT.search(text):
        return False
    return True


def looks_like_location(text: str) -> bool:
    """'ì¸ê³„ë™', 'ë…¼í˜„1ë™' ê°™ì€ ìœ„ì¹˜ ë¬¸ìì—´ì¸ì§€ ëŒ€ì¶© íŒë‹¨."""
    text = text.replace("\u00a0", " ").strip()

    if not text:
        return False
    if "|" in text:
        return False
    if "ì›" in text:
        return False
    if looks_like_time(text):
        return False
    # ë„ˆë¬´ ê¸¸ë©´ ì œëª©ì¼ í™•ë¥  ë†’ìŒ
    if len(text) > 15:
        return False

    # ë™/êµ¬/ì‹œ/ì/ë©´/ë¦¬ ê°™ì€ ì§€ëª… ì ‘ë¯¸ì‚¬ í¬í•¨í•˜ë©´ ìœ„ì¹˜ì¼ í™•ë¥  ë†’ìŒ
    if any(suffix in text for suffix in ["ë™", "êµ¬", "ì‹œ", "ì", "ë©´", "ë¦¬"]):
        return True

    return False


# ================== ìƒì„¸ í˜ì´ì§€ íŒŒì„œ (ì´ë¦„/ê°€ê²©ë§Œ) ==================


def parse_product_page(url: str, save_html: bool = False) -> Optional[dict]:
    """ìƒí’ˆ ìƒì„¸í˜ì´ì§€ì—ì„œ name/price/time/location ì¶”ì¶œ"""
    try:
        resp = requests.get(url, headers=HEADERS, timeout=20)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        # ë””ë²„ê·¸: ì²« ìƒì„¸ í˜ì´ì§€ HTML ì €ì¥
        if save_html:
            with open("first_product_detail.html", "w", encoding="utf-8") as f:
                f.write(resp.text)
            print(f"  ğŸ’¾ ìƒì„¸ í˜ì´ì§€ HTML ì €ì¥ë¨: first_product_detail.html")

        # JSON-LD(Product) ìš°ì„ 
        jl = _parse_jsonld_product(soup)
        name = jl.get("name")
        price = jl.get("price")

        # í…ìŠ¤íŠ¸ ë°±ì—… (ìƒí’ˆëª…/ê°€ê²©)
        if not name:
            h1 = soup.select_one("h1")
            name = h1.get_text(strip=True) if h1 else "ìƒí’ˆëª…ì—†ìŒ"
        if price is None:
            p2 = _extract_price_from_text(soup)
            price = int(p2) if p2 is not None else 0

        # ì‹œê°„ê³¼ ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ ì‹œë„
        time_val = "ì‹œê°„ì—†ìŒ"
        location = "ì§€ì—­ì—†ìŒ"

        # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì‹œê°„ ì •ë³´ ì°¾ê¸°
        full_text = soup.get_text(" ", strip=True)
        time_match = TIME_PAT.search(full_text)
        if time_match:
            time_val = time_match.group(0)

        # span.text-gray-400 ê°™ì€ ìš”ì†Œë“¤ì—ì„œ ìœ„ì¹˜/ì‹œê°„ ì°¾ê¸°
        gray_spans = soup.select("span.text-gray-400, span.text-sm")
        for s in gray_spans:
            txt = s.get_text(strip=True)
            if not txt or txt == "|":
                continue

            if looks_like_time(txt):
                time_val = txt
            elif looks_like_location(txt) and location == "ì§€ì—­ì—†ìŒ":
                location = txt

        # ë””ë²„ê·¸ ì¶œë ¥
        if save_html:
            print(f"\n  [ìƒì„¸í˜ì´ì§€ ë””ë²„ê·¸]")
            print(f"    ì‹œê°„: {time_val}")
            print(f"    ìœ„ì¹˜: {location}")
            print(f"    ì „ì²´ í…ìŠ¤íŠ¸ ì• 500ì: {full_text[:500]}")
            print(f"    gray_spans ê°œìˆ˜: {len(gray_spans)}")

        return {
            "name": name or "ìƒí’ˆëª…ì—†ìŒ",
            "price": int(price)
            if isinstance(price, (int, float, str)) and str(price).isdigit()
            else (price or 0),
            "time": time_val,
            "location": location,
        }

    except Exception as e:
        print(f"âŒ {url} íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None


# ================== API ê¸°ë°˜ ê²€ìƒ‰ ==================


def fetch_search_api(keyword: str, page: int = 0) -> Optional[dict]:
    """
    ì¤‘ê³ ë‚˜ë¼ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    - pageëŠ” 0ë¶€í„° ì‹œì‘
    """
    params = {
        "keyword": keyword,
        "page": page,
        "pageSize": 40,  # í•œ í˜ì´ì§€ë‹¹ ìƒí’ˆ ê°œìˆ˜
        "sort": "RECENT",  # RECENT, LOW_PRICE, HIGH_PRICE, POPULAR
    }

    print(f"ğŸŒ API ê²€ìƒ‰ ìš”ì²­: {API_SEARCH_URL} (page={page})")
    try:
        resp = requests.get(API_SEARCH_URL, headers=API_HEADERS, params=params, timeout=20)
        resp.raise_for_status()
        data = resp.json()
        return data
    except Exception as e:
        print(f"âŒ API ìš”ì²­ ì‹¤íŒ¨ (page={page}): {e}")
        return None


def extract_products_from_api(data: dict, debug: bool = False) -> List[Dict[str, str]]:
    """
    API ì‘ë‹µì—ì„œ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ
    """
    products: List[Dict[str, str]] = []

    if not data or "data" not in data:
        return products

    items = data.get("data", {}).get("items", [])

    for idx, item in enumerate(items):
        product_id = item.get("productSeq") or item.get("seq") or item.get("id")
        if not product_id:
            continue

        name = item.get("title") or item.get("productTitle") or "ìƒí’ˆëª…ì—†ìŒ"
        price = item.get("price", 0)
        location = item.get("town") or item.get("location") or "ì§€ì—­ì—†ìŒ"

        # ì‹œê°„ ì •ë³´ ì¶”ì¶œ
        time_val = "ì‹œê°„ì—†ìŒ"
        # createdAt, updatedAt, publishedAt ë“±ì˜ í•„ë“œê°€ ìˆì„ ìˆ˜ ìˆìŒ
        for time_field in ["timeAgo", "time", "createdAt", "updatedAt", "publishedAt"]:
            if item.get(time_field):
                time_val = str(item[time_field])
                break

        link = f"https://web.joongna.com/product/{product_id}"

        # ë””ë²„ê¹… ëª¨ë“œ: ì²˜ìŒ 3ê°œ ìƒí’ˆì˜ ì›ë³¸ ë°ì´í„° ì¶œë ¥
        if debug and idx < 3:
            print(f"\n[DEBUG API] ìƒí’ˆ #{idx + 1}")
            print(f"  ì›ë³¸ ë°ì´í„° í‚¤: {list(item.keys())}")
            print(f"  ì´ë¦„: {name}")
            print(f"  ê°€ê²©: {price}")
            print(f"  ìœ„ì¹˜: {location}")
            print(f"  ì‹œê°„: {time_val}")
            print(f"  ë§í¬: {link}")

        products.append({
            "name": name,
            "price": int(price) if isinstance(price, (int, float)) else 0,
            "location": location,
            "time": time_val,
            "link": link,
        })

    print(f"   â””â”€ APIì—ì„œ ìƒí’ˆ {len(products)}ê°œ ì¶”ì¶œ")
    return products


# ================== Selenium ê¸°ë°˜ ê²€ìƒ‰ ==================


def fetch_search_page_selenium(driver: webdriver.Chrome, keyword: str, page: int = 1, save_html: bool = False) -> Optional[str]:
    """
    Seleniumì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
    - JavaScript ë Œë”ë§ ì™„ë£Œ í›„ HTML ë°˜í™˜
    """
    encoded_keyword = quote(keyword)
    url = BASE_SEARCH_URL.format(keyword=encoded_keyword)

    if page > 1:
        url = f"{url}&page={page}"

    print(f"ğŸŒ ê²€ìƒ‰ í˜ì´ì§€ ìš”ì²­ (Selenium): {url}")

    try:
        driver.get(url)

        # ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ê°€ ë¡œë”©ë  ë•Œê¹Œì§€ ëŒ€ê¸°
        wait = WebDriverWait(driver, 10)
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "ul.grid li")))

        # JavaScript ì‹¤í–‰ ì™„ë£Œë¥¼ ìœ„í•´ ì¶”ê°€ ëŒ€ê¸°
        time.sleep(2)

        html = driver.page_source

        # ë””ë²„ê·¸: HTML ì €ì¥
        if save_html and page == 1:
            with open(f"{keyword}_search_page_selenium.html", "w", encoding="utf-8") as f:
                f.write(html)
            print(f"  ğŸ’¾ HTML ì €ì¥ë¨: {keyword}_search_page_selenium.html")

        return html

    except TimeoutException:
        print(f"âŒ ê²€ìƒ‰ í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ (page={page})")
        return None
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨ (page={page}): {e}")
        return None


# ================== ê²€ìƒ‰ í˜ì´ì§€ HTML íŒŒì„œ (requests ë²„ì „) ==================


def fetch_search_page_html(keyword: str, page: int = 1, save_html: bool = False) -> Optional[str]:
    """
    ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ HTML ìš”ì²­
    - 1í˜ì´ì§€: /search/í‚¤ì›Œë“œ?keywordSource=INPUT_KEYWORD
    - 2í˜ì´ì§€~: /search/í‚¤ì›Œë“œ?keywordSource=INPUT_KEYWORD&page=2
    """
    encoded_keyword = quote(keyword)  # "ì•„ì´í°" -> "%EC%95%84%EC%9D%B4%ED%8F%B0"
    url = BASE_SEARCH_URL.format(keyword=encoded_keyword)

    # 2í˜ì´ì§€ ì´ìƒì¼ ë•Œë§Œ &page= ë¶™ì´ê¸°
    if page > 1:
        url = f"{url}&page={page}"

    print(f"ğŸŒ ê²€ìƒ‰ í˜ì´ì§€ ìš”ì²­: {url}")
    try:
        resp = requests.get(url, headers=HEADERS, timeout=20)
        resp.raise_for_status()

        # ë””ë²„ê·¸: HTML ì €ì¥
        if save_html and page == 1:
            with open(f"{keyword}_search_page.html", "w", encoding="utf-8") as f:
                f.write(resp.text)
            print(f"  ğŸ’¾ HTML ì €ì¥ë¨: {keyword}_search_page.html")

        return resp.text
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨ (page={page}): {e}")
        return None


def _extract_location_time_from_li(li: BeautifulSoup) -> Tuple[str, str]:
    """
    li ì¹´ë“œ í•˜ë‚˜ì—ì„œ ìœ„ì¹˜ / ì‹œê°„ í…ìŠ¤íŠ¸ ì¶”ì¶œ.
    - div.mt-1.mb-2 ì•ˆì˜ spanë“¤ì—ì„œ ìœ„ì¹˜/ì‹œê°„ ì •ë³´ë¥¼ ì°¾ëŠ”ë‹¤.
    """
    location = "ì§€ì—­ì—†ìŒ"
    time_val = "ì‹œê°„ì—†ìŒ"

    # ë°©ë²• 1: íŠ¹ì • div(mt-1 mb-2) ì•ˆì˜ spanë“¤ ì°¾ê¸°
    # <div class="mt-1 mb-2 min-h-6 max-lg:mb-0 max-lg:mt-1.5">
    info_div = li.select_one("div.mt-1.mb-2, div[class*='mt-1'][class*='mb-2']")
    if info_div:
        spans = info_div.find_all("span")
        for s in spans:
            txt = s.get_text(strip=True)
            if not txt or txt == "|":
                continue

            # ì‹œê°„ íŒ¨í„´ ì²´í¬ (ìš°ì„ ìˆœìœ„)
            if looks_like_time(txt):
                time_val = txt
            # ìœ„ì¹˜ íŒ¨í„´ ì²´í¬
            elif looks_like_location(txt) and location == "ì§€ì—­ì—†ìŒ":
                location = txt

    # ë°©ë²• 2: ëª¨ë“  spanì—ì„œ text-gray-400 í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ê²ƒë“¤ ì°¾ê¸°
    if location == "ì§€ì—­ì—†ìŒ" or time_val == "ì‹œê°„ì—†ìŒ":
        gray_spans = li.select("span.text-gray-400")
        for s in gray_spans:
            txt = s.get_text(strip=True)
            if not txt or txt == "|":
                continue

            if time_val == "ì‹œê°„ì—†ìŒ" and looks_like_time(txt):
                time_val = txt
            elif location == "ì§€ì—­ì—†ìŒ" and looks_like_location(txt):
                location = txt

    # ë°©ë²• 3: ëª¨ë“  span ê²€ìƒ‰
    if location == "ì§€ì—­ì—†ìŒ" or time_val == "ì‹œê°„ì—†ìŒ":
        span_texts: List[str] = []
        for s in li.find_all("span"):
            txt = s.get_text(strip=True)
            if not txt or txt == "|":
                continue
            span_texts.append(txt)

        if time_val == "ì‹œê°„ì—†ìŒ":
            time_candidates = [t for t in span_texts if looks_like_time(t)]
            if time_candidates:
                # ì—¬ëŸ¬ ê°œë©´ ì œì¼ ë§ˆì§€ë§‰ì„ ì‹œê°„ìœ¼ë¡œ
                time_val = time_candidates[-1]

        if location == "ì§€ì—­ì—†ìŒ":
            loc_candidates = [t for t in span_texts if looks_like_location(t)]
            if loc_candidates:
                location = loc_candidates[0]

    # ë°©ë²• 4: ê·¸ë˜ë„ ì‹œê°„ ëª» ì°¾ì•˜ìœ¼ë©´ li ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì •ê·œì‹ìœ¼ë¡œ íƒìƒ‰
    if time_val == "ì‹œê°„ì—†ìŒ":
        full_txt = li.get_text(" ", strip=True)
        match = TIME_PAT.search(full_txt)
        if match:
            time_val = match.group(0)

    return location, time_val


def extract_products_from_search(html: str, debug: bool = False) -> List[Dict[str, str]]:
    """
    ê²€ìƒ‰ ê²°ê³¼ HTMLì—ì„œ ìƒí’ˆ ì¹´ë“œ(li)ë³„ë¡œ
    - link
    - location
    - time
    ì„ ì¶”ì¶œ.
    """
    soup = BeautifulSoup(html, "html.parser")
    products: List[Dict[str, str]] = []
    seen: set[str] = set()

    # grid ì•ˆì˜ liê°€ ê°ê° ì¹´ë“œ
    for idx, li in enumerate(soup.select("ul.grid li")):
        a = li.select_one("a[href*='/product/']")
        if not a:
            continue

        href = a.get("href", "")
        if not href:
            continue

        # ìƒí’ˆì´ ì•„ë‹Œ ë“±ë¡ í˜ì´ì§€ ë“±ì€ ì œì™¸
        if "/product/form" in href:
            continue

        full_url = urljoin("https://web.joongna.com", href)
        if full_url in seen:
            continue
        seen.add(full_url)

        location, time_val = _extract_location_time_from_li(li)

        # ë””ë²„ê¹… ëª¨ë“œ: ì²˜ìŒ 3ê°œ ìƒí’ˆì˜ HTML êµ¬ì¡° ì¶œë ¥
        if debug and idx < 3:
            print(f"\n[DEBUG] ìƒí’ˆ #{idx + 1}")
            print(f"  ìœ„ì¹˜: {location}")
            print(f"  ì‹œê°„: {time_val}")
            print(f"  ë§í¬: {full_url}")

            # ì²« ë²ˆì§¸ ìƒí’ˆì˜ ì „ì²´ HTML ì €ì¥
            if idx == 0:
                with open(f"first_product_card.html", "w", encoding="utf-8") as f:
                    f.write(li.prettify())
                print(f"  ğŸ’¾ ì²« ìƒí’ˆ ì¹´ë“œ HTML ì €ì¥ë¨: first_product_card.html")

            # div.mt-1.mb-2 ì°¾ê¸°
            info_div = li.select_one("div.mt-1.mb-2, div[class*='mt-1'][class*='mb-2']")
            if info_div:
                print(f"  info_div í…ìŠ¤íŠ¸: {info_div.get_text(' ', strip=True)}")
                print(f"  info_div HTML: {info_div}")
                print(f"  info_div span ê°œìˆ˜: {len(info_div.find_all('span'))}")
            else:
                print(f"  info_divë¥¼ ì°¾ì§€ ëª»í•¨")

            # ëª¨ë“  span ì¶œë ¥
            all_spans = li.find_all("span")
            print(f"  ì „ì²´ span ê°œìˆ˜: {len(all_spans)}")
            for i, s in enumerate(all_spans[:10]):  # ì²˜ìŒ 10ê°œë§Œ
                txt = s.get_text(strip=True)
                classes = s.get("class", [])
                if txt:
                    print(f"    span[{i}]: '{txt}' | classes: {classes}")

        products.append(
            {
                "link": full_url,
                "location": location,
                "time": time_val,
            }
        )

    print(f"   â””â”€ ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ìƒí’ˆ ì¹´ë“œ {len(products)}ê°œ ì¶”ì¶œ")
    return products


# ================== ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜(API ë²„ì „) ==================


def crawl_search_api(keyword: str, limit: int = 200, sleep_range=(1.0, 3.0), debug: bool = False) -> List[dict]:
    """
    ì¤‘ê³ ë‚˜ë¼ APIë¥¼ ì‚¬ìš©í•œ ìƒí’ˆ í¬ë¡¤ë§
    - keyword: ê²€ìƒ‰í•  í‚¤ì›Œë“œ (ì˜ˆ: "ì•„ì´í°")
    - limit: ìˆ˜ì§‘í•  ìƒí’ˆ ê°œìˆ˜
    - debug: ë””ë²„ê·¸ ëª¨ë“œ
    """
    results: List[dict] = []
    page = 0

    print(f"ğŸ” '{keyword}' ê²€ìƒ‰ ê²°ê³¼ í¬ë¡¤ë§ ì‹œì‘ (API ë²„ì „)...\n")

    while len(results) < limit:
        print(f"ğŸ“„ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ {page + 1} ìš”ì²­ ì¤‘...")

        data = fetch_search_api(keyword, page=page)
        if not data:
            print("âš ï¸ API ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
            break

        # ì²« í˜ì´ì§€ë§Œ ë””ë²„ê·¸ ì¶œë ¥
        products = extract_products_from_api(data, debug=(debug and page == 0))
        if not products:
            print("âš ï¸ ë” ì´ìƒ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
            break

        for product in products:
            if len(results) >= limit:
                break

            print(
                f"âœ… {product['name']} / {product['price']}ì› / "
                f"{product['location']} / {product['time']}"
            )

            results.append(product)
            time.sleep(random.uniform(*sleep_range))

        page += 1
        time.sleep(random.uniform(*sleep_range))

    return results


# ================== ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜(Selenium ë²„ì „) ==================


def crawl_search_selenium(keyword: str, limit: int = 200, sleep_range=(1.0, 3.0), debug: bool = False, headless: bool = True) -> List[dict]:
    """
    Seleniumì„ ì‚¬ìš©í•œ ìƒí’ˆ í¬ë¡¤ë§
    - keyword: ê²€ìƒ‰í•  í‚¤ì›Œë“œ (ì˜ˆ: "ì•„ì´í°")
    - limit: ìˆ˜ì§‘í•  ìƒí’ˆ ê°œìˆ˜
    - debug: ë””ë²„ê·¸ ëª¨ë“œ
    - headless: Trueë©´ ë¸Œë¼ìš°ì € ì°½ì„ ë„ìš°ì§€ ì•ŠìŒ
    """
    results: List[dict] = []
    page = 1

    print(f"ğŸ” '{keyword}' ê²€ìƒ‰ ê²°ê³¼ í¬ë¡¤ë§ ì‹œì‘ (Selenium ë²„ì „)...\n")

    # WebDriver ìƒì„±
    driver = create_driver(headless=headless)

    try:
        while len(results) < limit:
            print(f"ğŸ“„ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ {page} ìš”ì²­ ì¤‘...")

            # ì²« í˜ì´ì§€ë§Œ HTML ì €ì¥
            save_html = (page == 1 and globals().get('SAVE_HTML', False))
            html = fetch_search_page_selenium(driver, keyword, page=page, save_html=save_html)
            if not html:
                print("âš ï¸ ê²€ìƒ‰ í˜ì´ì§€ ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
                break

            # ì²« í˜ì´ì§€ë§Œ ë””ë²„ê·¸ ì¶œë ¥
            product_cards = extract_products_from_search(html, debug=(debug and page == 1))
            if not product_cards:
                print("âš ï¸ ë” ì´ìƒ ìƒí’ˆ ì¹´ë“œê°€ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
                break

            for card in product_cards:
                if len(results) >= limit:
                    break

                link = card["link"]
                location_from_search = card["location"]
                time_from_search = card["time"]

                # ì²« ìƒí’ˆë§Œ HTML ì €ì¥
                save_detail_html = (len(results) == 0 and globals().get('SAVE_HTML', False))
                detail = parse_product_page(link, save_html=save_detail_html)
                if not detail:
                    continue

                # ìƒì„¸ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜¨ ì •ë³´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ê²€ìƒ‰ í˜ì´ì§€ ì •ë³´ ì‚¬ìš©
                location = detail.get("location", "ì§€ì—­ì—†ìŒ")
                if location == "ì§€ì—­ì—†ìŒ":
                    location = location_from_search

                time_val = detail.get("time", "ì‹œê°„ì—†ìŒ")
                if time_val == "ì‹œê°„ì—†ìŒ":
                    time_val = time_from_search

                row = {
                    "name": detail["name"],
                    "price": detail["price"],
                    "location": location,
                    "time": time_val,
                    "link": link,
                }

                print(
                    f"âœ… {row['name']} / {row['price']}ì› / "
                    f"{row['location']} / {row['time']}"
                )

                results.append(row)

                time.sleep(random.uniform(*sleep_range))  # ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ê°„ ë”œë ˆì´

            page += 1
            time.sleep(random.uniform(*sleep_range))  # í˜ì´ì§€ ì „í™˜ ë”œë ˆì´

    finally:
        # WebDriver ì¢…ë£Œ
        driver.quit()
        print("\nğŸ”’ WebDriver ì¢…ë£Œ")

    return results


# ================== ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜(HTML ë²„ì „ - requests) ==================


def crawl_search_results(keyword: str, limit: int = 200, sleep_range=(1.0, 3.0), debug: bool = False) -> List[dict]:
    """
    ê²€ìƒ‰ í‚¤ì›Œë“œ ê¸°ë°˜ ìƒí’ˆ í¬ë¡¤ë§ (HTML ë²„ì „)
    - keyword: ê²€ìƒ‰í•  í‚¤ì›Œë“œ (ì˜ˆ: "ì•„ì´í°")
    - limit: ìˆ˜ì§‘í•  ìƒí’ˆ ê°œìˆ˜
    - debug: ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™” (ì²˜ìŒ ëª‡ ê°œ ìƒí’ˆì˜ ìƒì„¸ ì •ë³´ ì¶œë ¥)
    """
    results: List[dict] = []
    page = 1

    print(f"ğŸ” '{keyword}' ê²€ìƒ‰ ê²°ê³¼ í¬ë¡¤ë§ ì‹œì‘...\n")

    while len(results) < limit:
        print(f"ğŸ“„ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ {page} ìš”ì²­ ì¤‘...")

        # ì²« í˜ì´ì§€ë§Œ HTML ì €ì¥
        save_html = (page == 1 and globals().get('SAVE_HTML', False))
        html = fetch_search_page_html(keyword, page=page, save_html=save_html)
        if not html:
            print("âš ï¸ ê²€ìƒ‰ í˜ì´ì§€ ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
            break

        # ì²« í˜ì´ì§€ë§Œ ë””ë²„ê·¸ ì¶œë ¥
        product_cards = extract_products_from_search(html, debug=(debug and page == 1))
        if not product_cards:
            print("âš ï¸ ë” ì´ìƒ ìƒí’ˆ ì¹´ë“œê°€ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
            break

        for card in product_cards:
            if len(results) >= limit:
                break

            link = card["link"]
            location_from_search = card["location"]
            time_from_search = card["time"]

            # ì²« ìƒí’ˆë§Œ HTML ì €ì¥
            save_detail_html = (len(results) == 0 and globals().get('SAVE_HTML', False))
            detail = parse_product_page(link, save_html=save_detail_html)
            if not detail:
                continue

            # ìƒì„¸ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜¨ ì •ë³´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ê²€ìƒ‰ í˜ì´ì§€ ì •ë³´ ì‚¬ìš©
            location = detail.get("location", "ì§€ì—­ì—†ìŒ")
            if location == "ì§€ì—­ì—†ìŒ":
                location = location_from_search

            time_val = detail.get("time", "ì‹œê°„ì—†ìŒ")
            if time_val == "ì‹œê°„ì—†ìŒ":
                time_val = time_from_search

            row = {
                "name": detail["name"],
                "price": detail["price"],
                "location": location,
                "time": time_val,
                "link": link,
            }

            print(
                f"âœ… {row['name']} / {row['price']}ì› / "
                f"{row['location']} / {row['time']}"
            )

            results.append(row)

            time.sleep(random.uniform(*sleep_range))  # ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ê°„ ë”œë ˆì´

        page += 1
        time.sleep(random.uniform(*sleep_range))  # í˜ì´ì§€ ì „í™˜ ë”œë ˆì´

    return results


# ================== ì‹¤í–‰ ì§„ì…ì  ==================

if __name__ == "__main__":
    # ì»¤ë§¨ë“œ ë¼ì¸ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description="ì¤‘ê³ ë‚˜ë¼ í¬ë¡¤ëŸ¬")
    parser.add_argument("-k", "--keyword", type=str, default="ì•„ì´í°", help="ê²€ìƒ‰ í‚¤ì›Œë“œ (ê¸°ë³¸ê°’: ì•„ì´í°)")
    parser.add_argument("-l", "--limit", type=int, default=50, help="ìˆ˜ì§‘í•  ìƒí’ˆ ê°œìˆ˜ (ê¸°ë³¸ê°’: 50)")
    parser.add_argument("-d", "--debug", action="store_true", help="ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”")
    parser.add_argument("--no-headless", action="store_true", help="ë¸Œë¼ìš°ì € ì°½ í‘œì‹œ")
    parser.add_argument("--save-html", action="store_true", help="HTML íŒŒì¼ ì €ì¥")
    parser.add_argument("--no-selenium", action="store_true", help="requests ì‚¬ìš© (Selenium ë¹„í™œì„±í™”)")
    parser.add_argument("--save-db", action="store_true", help="ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥")
    parser.add_argument("--category", type=str, default="iPhone", help="ì¹´í…Œê³ ë¦¬ëª… (ê¸°ë³¸ê°’: iPhone)")
    parser.add_argument("--no-csv", action="store_true", help="CSV íŒŒì¼ ì €ì¥ ì•ˆ í•¨")

    args = parser.parse_args()

    KEYWORD = args.keyword
    LIMIT = args.limit
    DEBUG = args.debug
    USE_SELENIUM = not args.no_selenium
    HEADLESS = not args.no_headless
    SAVE_HTML = args.save_html

    print("=" * 60)
    if USE_SELENIUM:
        print(f"ì¤‘ê³ ë‚˜ë¼ '{KEYWORD}' ê²€ìƒ‰ í¬ë¡¤ë§ (Selenium ë²„ì „)")
    else:
        print(f"ì¤‘ê³ ë‚˜ë¼ '{KEYWORD}' ê²€ìƒ‰ í¬ë¡¤ë§ (requests ë²„ì „)")
    print(f"ìˆ˜ì§‘ ê°œìˆ˜: {LIMIT}ê°œ")
    print("=" * 60 + "\n")

    if USE_SELENIUM:
        data = crawl_search_selenium(
            keyword=KEYWORD,
            limit=LIMIT,
            sleep_range=(0.5, 1.5),
            debug=DEBUG,
            headless=HEADLESS
        )
    else:
        data = crawl_search_results(keyword=KEYWORD, limit=LIMIT, sleep_range=(0.5, 1.5), debug=DEBUG)

    if not data:
        print(f"\n'{KEYWORD}' ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        raise SystemExit(0)

    print("\n" + "=" * 60)
    print(f"âœ¨ ì´ {len(data)}ê°œì˜ '{KEYWORD}' ìƒí’ˆì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    print("=" * 60 + "\n")

    # CSV ì €ì¥
    if not args.no_csv:
        df = pd.DataFrame(data)
        if USE_SELENIUM:
            out_csv = f"{KEYWORD}_products_selenium.csv"
            version_text = "Selenium ë²„ì „"
        else:
            out_csv = f"{KEYWORD}_products_requests.csv"
            version_text = "requests ë²„ì „"

        df.to_csv(out_csv, encoding="utf-8-sig", index=False)
        print(f"ğŸ“ CSV ì €ì¥ ì™„ë£Œ: {os.path.abspath(out_csv)} ({version_text})")

    # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
    if args.save_db:
        try:
            from db_manager import DatabaseManager

            print("\n" + "=" * 60)
            print("ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...")
            print("=" * 60 + "\n")

            db = DatabaseManager()
            success_count = db.insert_items_batch(
                products=data,
                marketplace_code="joongna",
                category_name=args.category
            )
            db.close()

            print("\n" + "=" * 60)
            print(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ: {success_count}/{len(data)}ê°œ")
            print("=" * 60)

        except ImportError:
            print("âŒ db_manager.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")

    print("\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
